{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc65a7c5",
   "metadata": {},
   "source": [
    "## **概括**\n",
    "\n",
    "目前还有宝箱与Buff的奖励仍未加入，为了使agent的得分尽可能的高，需要在到达终点的前提下，尽量收集完所有宝箱，同时为了使模型有一个比较好的能力，采用分阶段训练，第一阶段先训练模型到达终点，随后进行融合，在到达终点的同时考虑宝箱的收集，最后阶段完全考虑宝箱的收集。\n",
    "\n",
    "特征计算方面，引入了最近宝箱和buff的特征14维(格式同终点)，4维的额外信息(步数比例，宝箱收集比例，每步得分，buff收集，7*7障碍密度)，以及多宝箱--终点联合特征(最近的3个宝箱到终点的距离以及角度)。同时为了使模型更好的找到宝箱，加快收敛速度，引入了潜势函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f362fa",
   "metadata": {},
   "source": [
    "### **特征维度**\n",
    "\n",
    "1. 新引入的14维宝箱与buff特征，均调用`_get_pos_feature`函数，内部具体特征同之前\n",
    "\n",
    "```python\n",
    "        # 奖励检测\n",
    "        cur_treasure_count = obs[\"score_info\"][\"treasure_collected_count\"]\n",
    "        cur_buff_count = obs[\"score_info\"][\"buff_count\"]\n",
    "        self.treasure_gain = cur_treasure_count - self.prev_treasure_count\n",
    "        self.buff_gain = cur_buff_count - self.prev_buff_count\n",
    "        self.prev_treasure_count = cur_treasure_count\n",
    "        self.prev_buff_count = cur_buff_count\n",
    "\n",
    "        # 寻找宝箱\n",
    "        visible_treasures = [org for org in obs[\"frame_state\"][\"organs\"] \n",
    "                              if org[\"sub_type\"] == 1 and org[\"status\"] == 1]\n",
    "        self.visible_treasures = [\n",
    "            (org[\"pos\"][\"x\"], org[\"pos\"][\"z\"]) for org in visible_treasures\n",
    "        ]\n",
    "        if visible_treasures:\n",
    "            # 最近宝箱\n",
    "            nearest = min(visible_treasures, key=lambda o: \n",
    "                          (o[\"pos\"][\"x\"]-self.cur_pos[0])**2 + (o[\"pos\"][\"z\"]-self.cur_pos[1])**2)\n",
    "            target = (nearest[\"pos\"][\"x\"], nearest[\"pos\"][\"z\"])\n",
    "            self.feature_treasure = self._get_pos_feature(1, self.cur_pos, target)\n",
    "            self.cur_treasure_dist = np.linalg.norm(np.array(self.cur_pos) - np.array(target))\n",
    "        else:\n",
    "            # 未发现宝箱\n",
    "            self.feature_treasure = np.concatenate([\n",
    "                [0.0], np.zeros(8), np.zeros(2), np.zeros(2), [1.0]\n",
    "            ], dtype=np.float32)\n",
    "            self.cur_treasure_dist = None\n",
    "        # 寻找Buff\n",
    "        buff_obj = next((org for org in obs[\"frame_state\"][\"organs\"] \n",
    "                         if org[\"sub_type\"] == 2 and org[\"status\"] == 1), None)\n",
    "        if buff_obj:\n",
    "            bpos = (buff_obj[\"pos\"][\"x\"], buff_obj[\"pos\"][\"z\"])\n",
    "            self.feature_buff = self._get_pos_feature(1, self.cur_pos, bpos)\n",
    "        else:\n",
    "            self.feature_buff = np.concatenate([\n",
    "                [0.0], np.zeros(8), np.zeros(2), np.zeros(2), [1.0]\n",
    "            ], dtype=np.float32)\n",
    "```\n",
    "\n",
    "\n",
    "2. 额外信息\n",
    "\n",
    "- 分数信息，`self.score_delta`用于表征当前步数的得分变化\n",
    "```python\n",
    "cur_score = obs[\"score_info\"][\"score\"]\n",
    "self.score_delta = self.prev_score - cur_score\n",
    "self.prev_score = cur_score\n",
    "```\n",
    "\n",
    "- 步数，宝箱，buff全局信息，注意使用归一化，传入`extra_feats`中\n",
    "```python\n",
    "step_ratio = obs[\"score_info\"][\"step_no\"] /  Config.MAX_STEP\n",
    "treasure_ratio = obs[\"score_info\"][\"treasure_collected_count\"] / Config.TOTAL_TREASURES\n",
    "score_delta = self.score_delta\n",
    "talent_count = obs[\"score_info\"][\"talent_count\"] / 100\n",
    "```\n",
    "\n",
    "- 7*7障碍密度信息\n",
    "```python\n",
    "h, w = self.local_map.shape\n",
    "center_y, center_x = h // 2, w // 2\n",
    "r = 3\n",
    "y0 = max(0, center_y - r)\n",
    "y1 = min(h, center_y + r + 1)\n",
    "x0 = max(0, center_x - r)\n",
    "x1 = min(w, center_x + r + 1)\n",
    "submap = self.local_map[y0:y1, x0:x1]\n",
    "if submap.size > 0:\n",
    "    density_7x7 = float(np.mean(submap != 0))\n",
    "else:\n",
    "    density_7x7 = 0.0\n",
    "    extra_feats.append(density_7x7)\n",
    "```\n",
    "\n",
    "\n",
    "3. 宝箱终点联合信息，返回归一化的两个距离以及角度\n",
    "```python\n",
    "cx, cz = self.cur_pos\n",
    "t2e_feats = []\n",
    "# 距离排序\n",
    "vt = sorted(self.visible_treasures,\n",
    "            key=lambda p: (p[0]-cx)**2+(p[1]-cz)**2)[:3]\n",
    "for bx, bz in vt:\n",
    "    dx1, dz1 = bx-cx, bz-cz\n",
    "    dx2, dz2 = self.end_pos[0]-bx, self.end_pos[1]-bz if self.end_pos else (0,0)\n",
    "    d1_norm = math.hypot(dx1, dz1)/self.max_map_dist\n",
    "    d2_norm = math.hypot(dx2, dz2)/self.max_map_dist\n",
    "    cos_ang = (dx1*dx2 + dz1*dz2) / (math.hypot(dx1,dz1)*math.hypot(dx2,dz2)+1e-6)\n",
    "    t2e_feats += [d1_norm, d2_norm, cos_ang]\n",
    "while len(t2e_feats)<9: t2e_feats.append(0.0)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad0b3d",
   "metadata": {},
   "source": [
    "### **潜势函数**\n",
    "\n",
    "设计势函数$\\varPhi \\left( s \\right) =-d$，得到\n",
    "$$shape\\ =\\ \\gamma \\varPhi \\left( s \\right) -\\varPhi \\left( s^{'} \\right) $$\n",
    "其中$\\varPhi \\left( s \\right)$为当前状态的势能，$\\varPhi \\left( s^{'} \\right)$为前一状态的势能，给宝箱和终点均设计潜势函数，可以引导agent向宝箱/终点靠近。(注意归一化)\n",
    "\n",
    "同时为了能控制分阶段训练，设计$t,e$权重值加权，最终势能公式\n",
    "$$shape = t \\times shape_t + e \\times shape_e$$\n",
    "\n",
    "具体代码如下：\n",
    "```python\n",
    "        shape_T = 0.0\n",
    "        if self.cur_treasure_dist is not None:\n",
    "            cur_td_norm = self.cur_treasure_dist / self.max_map_dist\n",
    "            cur_phi_t = -cur_td_norm\n",
    "            prev_phi_t = getattr(self, \"prev_treasure_phi\", 0.0)\n",
    "            shape_T     = Config.TREASURE_REWARD * (gamma_0 * cur_phi_t - prev_phi_t)\n",
    "            self.prev_treasure_phi = cur_phi_t\n",
    "        else:\n",
    "            self.prev_treasure_phi = 0.0\n",
    "        \n",
    "        shape_E = 0.0\n",
    "        if self.end_pos is not None:\n",
    "            raw_ed      = np.linalg.norm(np.array(self.cur_pos) - np.array(self.end_pos))\n",
    "            cur_ed_norm = raw_ed / self.max_map_dist\n",
    "            cur_phi_e   = -cur_ed_norm\n",
    "            prev_phi_e  = getattr(self, \"prev_end_phi\", 0.0)\n",
    "            shape_E     = Config.GOAL_REWARD * (gamma_0 * cur_phi_e - prev_phi_e)\n",
    "            self.prev_end_phi = cur_phi_e\n",
    "        else:\n",
    "            self.prev_end_phi = 0.0\n",
    "\n",
    "        shape = t * shape_T + e * shape_E\n",
    "```\n",
    "\n",
    "分别针对宝箱和终点设计一次性收集奖励和到达奖惩(t,e)均为上述的权重。此处为了使agent能尽可能手机玩宝箱，给出一个到达终点但是未收集完宝箱的惩罚 \n",
    "```python\n",
    "        one_time = Config.TREASURE_IMMEDIATE_REWARD * max(0, self.treasure_gain) * t\n",
    "        if done:\n",
    "            # 宝箱训练中，未收集完宝箱到终点\n",
    "            if t > 0.0 and obs[\"score_info\"][\"treasure_collected_count\"] < Config.TOTAL_TREASURES:\n",
    "                end_pen = Config.INCOMPLETE_END_PENALTY * t\n",
    "            # 宝箱训练中，收集完宝箱到终点\n",
    "            elif t > 0.0 and obs[\"score_info\"][\"treasure_collected_count\"] == Config.TOTAL_TREASURES:\n",
    "                end_pen = Config.GOAL_REWARD * e + Config.PERFECT_REWARD * e\n",
    "            # 终点训练奖励\n",
    "            else:\n",
    "                end_pen = Config.GOAL_REWARD * e\n",
    "        else:\n",
    "            end_pen = 0.0\n",
    "```\n",
    "\n",
    "同样的buff奖励也需要考虑，收集`buff_gain * 0.2`，可以调节0.2参数进行设置。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98164610",
   "metadata": {},
   "source": [
    "### **分阶段训练**\n",
    "\n",
    "为了使agent能更好的学会即收集宝箱又到达终点，采用分阶段训练策略，如概述所说，一阶段先训练模型到达终点，随后进行融合，在到达终点的同时考虑宝箱的收集，最后阶段完全考虑宝箱的收集。故需要对终点和宝箱的奖励进行加权，使用上述的$t, e$参数，分别代表宝箱权重和终点权重。\n",
    "\n",
    "在第一阶段，追逐终点$t=0.0, e=1.0$，第二阶段，开始动态融合，第三阶段$t=1.0, e=0.0$，同时为了防止在最后阶段agent完全忽视终点的问题，对参数$e$进行兜底，即$e=max(e, E_MIN)$，同时需要保证$t+e=1$具体代码如下：其中`Config.S1_STEPS`为阶段1的学习次数，`Config.S2_STEPS`为阶段2的学习次数(学习次数指的是agent进行learn的次数，具体在`train_workflow`中详细使用)\n",
    "```python\n",
    "if global_step < Config.S1_STEPS:\n",
    "    e, t = 1.0, 0.0\n",
    "elif global_step < Config.S2_STEPS:\n",
    "    e = 1 - (global_step - Config.S1_STEPS) / (Config.S2_STEPS - Config.S1_STEPS)\n",
    "else:\n",
    "    e, t= 0.0, 1.0\n",
    "gamma_0 = 0.99\n",
    "e = max(e, Config.E_MIN)\n",
    "treasure_left = Config.TOTAL_TREASURES - obs[\"score_info\"][\"treasure_collected_count\"]\n",
    "if treasure_left == 0:\n",
    "     e = 1.0\n",
    "elif treasure_left <= 2:\n",
    "    e = max(e, 0.6)\n",
    "t = 1 - e\n",
    "shape = t * shape_T + e * shape_E  \n",
    "```\n",
    "\n",
    "同时注意需要修改前面设计的终点相关奖励，传入一个$e$进行加权，防止影响\n",
    "```python\n",
    "near_goal_penalty = near_goal_penalty * e\n",
    "cone_reward = 0.3 * (0.3 - end_dist) if end_dist < 0.3 else 0.0\n",
    "cone_reward = cone_reward * e\n",
    "```\n",
    "\n",
    "分阶段训练使用的轮数放在了`train_workflow.py`文件中，新增`agent.global_step=0`计数，没学习一次则+1，如下(我设置的`episode_num_every_epoch`==1)，即每一局学习一次并且次数+1，在`obseration_process`中增加传入参数`agent.global_step`和`done`，进入传入`process()`中进行计算与分析。具体见`preprocessor.py`文件中的`process`函数。需要注意的是，在`exploit`中也使用了`observation_process`函数，但是由于评估模式下，不需要使用奖励设计，所以可以随便传入一个`agent.global_step`和`done`即可，否则会出现测试异常。\n",
    "```python\n",
    "while True:\n",
    "    for g_data, monitor_data in run_episodes(episode_num_every_epoch, env, agent,     usr_conf, logger, monitor, epoch):\n",
    "        agent.learn(g_data)\n",
    "        agent.global_step += 1\n",
    "        g_data.clear()\n",
    "```\n",
    "\n",
    "最后本版本的代码在监控数据中增加了权重$t$和$e$的具体数值，可以看到情况，以及`agent.global_step`的具体轮数，方便查看。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fa98d2",
   "metadata": {},
   "source": [
    "### **配置文件**\n",
    "\n",
    "为了统一参数的使用，部分需要更改调整的参数放在了`conf/conf.py`文件下\n",
    "```python\n",
    "# 训练调度\n",
    "S1_STEPS = 2500\n",
    "S2_STEPS = 8000\n",
    "MAX_STEP = 2000\n",
    "\n",
    "# 宝箱\n",
    "TOTAL_TREASURES = 8\n",
    "\n",
    "# 潜势函数\n",
    "TREASURE_REWARD = 2.5  # 潜势函数使用\n",
    "TREASURE_IMMEDIATE_REWARD = 2.5 # 收集宝箱立刻给\n",
    "\n",
    "GOAL_REWARD = 5.0\n",
    "INCOMPLETE_END_PENALTY = -3.5\n",
    "PERFECT_REWARD = 2.5\n",
    "\n",
    "REWARD_CLIP = 5.0\n",
    "\n",
    "E_MIN = 0.25\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c124636",
   "metadata": {},
   "source": [
    "### **问题**\n",
    "\n",
    "现在的核心问题还是在：agent要么能走到终点收集2-3个少量的宝箱，要么收集大量的6-8个宝箱，但是无法走到终点，两者很难做到平衡\n",
    "\n",
    "优化思路：\n",
    "1. 可以考虑做更多的特征维度，宝箱收集信息等，局部视野情况，可以参考社区的文章\n",
    "2. 继续调整参数，平衡宝箱和终点之间的收益，\n",
    "3. 模型可以继续优化修改(当前的reward和q_vlaues)仍然存在震荡，多少局学习也可以进行修改"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
