{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98afd317",
   "metadata": {},
   "source": [
    "## **项目理解**\n",
    "\n",
    "\n",
    "### **1 项目总结构**\n",
    "\n",
    "具体代码目录：\n",
    "\n",
    "```css\n",
    "📦 根目录\n",
    "├── 📂 agent\n",
    "│   ├── 📂 algorithm\n",
    "│       └── 📄 __init__.py\n",
    "│       └── 📄 algorithm.py\n",
    "│   ├── 📂 conf\n",
    "│       └── 📄 __init__.py\n",
    "│       └── 📄 conf.py\n",
    "│       └── 📄 train_env_conf.toml\n",
    "│   ├── 📂 feature\n",
    "│       └── 📄 __init__.py\n",
    "│       └── 📄 definition.py\n",
    "│       └── 📄 preprocessor.py\n",
    "│   ├── 📂 model\n",
    "│       └── 📄 __init__.py\n",
    "│       └── 📄 model.py\n",
    "│   ├── 📂 workflow\n",
    "│       └── 📄 __init__.py\n",
    "│       └── 📄 train_workflow.py\n",
    "│   ├── 📄 __init__.py\n",
    "│   └── 📄 agent.py\n",
    "├── 📂 conf\n",
    "│   ├── 📄 __init__.py\n",
    "│   ├── 📄 configure_app.toml\n",
    "├── 📂 log\n",
    "\n",
    "\n",
    "\n",
    "└── 📄 train_test.py\n",
    "```\n",
    "\n",
    "具体作用如下：\n",
    "\n",
    "![总描述](image/5.png)\n",
    "\n",
    "![agent目录](image/6.png)\n",
    "\n",
    "![conf目录](image/7.png)\n",
    "\n",
    "\n",
    "### **2 环境系统总述**\n",
    "\n",
    "有两个接口函数，注意这两个接口函数的实现以及具体的调用都是由运行时注入并通过`RPC/HTTP`等方式调用远端服务的，故只需要了解其的传入参数以及返回参数即可，如下：\n",
    "\n",
    "\n",
    "#### **2.1 接口函数**\n",
    "\n",
    "-  `env.reset(usr_conf: dict) → (obs, state)`根据`train_env_conf.toml`中的参数初始化环境，返回初始观测与全局状态\n",
    "-  `env.step(act: int) → (frame_no, obs, score, terminated, truncated, extra_info)`接受动作(0: UP, 1: DOWN, 2: LEFT, 3: RIGHT)，执行一步环境转移，返回当前帧编号，观测，即时得分，是否结束，是否异常中断及额外信息\n",
    "\n",
    "#### **2.2 返回数据结构**\n",
    "\n",
    "- `obs`：包含`feature`一维特征向量与`legal_act`合法动作列表\n",
    "- `extra_info`，包括：\n",
    "    - `frame_state`当前环境内部状态\n",
    "    - `game_info`：`pos_x/pos_z`、`step_no、treasure_count`、`treasure_status`、`local_view`（5×5局部视野展平）及 `location_memory`（64×64全局记忆展平）\n",
    "    - `score_info`：即时与累计得分\n",
    "\n",
    "\n",
    "### **3 智能体执行流程**\n",
    "\n",
    "#### **3.1 环境模型**\n",
    "\n",
    "项目提供了一个预先计算好的状态转移字典F(存储于`conf/map_data/F_level_1.json`)，描述了每个状态下执行个动作的结果。`F`的结构是一个嵌套字典：键为状态编号，值为该状态下各动作的结果；每个动作结果包括下一个状态、获得的奖励以及是否结束标志。\n",
    "\n",
    "#### **3.2 算法流程**\n",
    "\n",
    "该模块实现了值迭代和策略迭代两种方法，默认采用值迭代\n",
    "\n",
    "- 算法初始化时，初始化一个随机均匀策略表\n",
    "```python\n",
    "self.agent_policy = np.ones([self.state_size, self.action_size]) / self.action_size\n",
    "```\n",
    "\n",
    "- 进入算法具体实现：\n",
    "\n",
    "值迭代核心函数：\n",
    "\n",
    "$$ V_{k+1}\\left( s \\right) =\\underset{a\\in \\mathcal{A}}{\\max}\\left[ R\\left( s,a \\right) +\\gamma P\\left( s^{\\prime}|s,a \\right) V_k\\left( s^{\\prime} \\right) \\right] $$\n",
    "\n",
    "在确定的环境下，转移概率$ P\\left( s^{\\prime}|s,a \\right) $为$1$或$0$，公式化简为：\n",
    "\n",
    "$$ V_{k+1}(s) = \\max_{a} \\left[ R(s,a) + \\gamma V_k(s^{\\prime}) \\right]$$\n",
    "\n",
    "当所有状态更新后的最大差值$\\max _s|V_{k+1}\\left( s \\right) -V_k\\left( s \\right) |$小于阈值$\\theta$，认为收敛，取$V^{*}=V_{k+1}$。此时最优策略可由：\n",
    "\n",
    "$$\\pi^{\\star}(s) = \\arg\\max_{a} \\left[ R(s,a) + \\gamma V^{\\star}(s^{\\prime}) \\right]$$\n",
    "\n",
    "直接导出\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
